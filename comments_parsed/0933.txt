How AI recognizes things and computes them is definitely strange to us, but it's not so cut and dry as you're making out. The case that it's doing the same basic thing that we do, pattern matching, is the most convincing. The largest difference between us exist at higher levels, where we have all this evolutionary architecture built up on top of that basic function. The case where we work in a fundamentally similar way, but humans have all this evolutionary architecture built into us that modifies our output (as opposed to our functions), is actually quite strong"
"Right, but what we have with chatgpt is not that by a long shot."
"You're trying to use theoretical ideals and possible outcomes, and attribute those to an existing system.  Just because something is theoretically possible (and I'm not saying it is), doesn't mean it currently exists.  Even if an LLM could evolve to reason and learn, that doesn't mean ChatGPT can currently do so.
When we talk about ""learning"", and it's parallels to humans, you need to distinguish between memorizing and extrapolating from thinking and reasoning.  ChatGPT maybe terrific at memorizing - and with a large enough dataset might be quite good at extrapolating - but it's not thinking nor reasoning.  It can't create new knowledge.

