But it’s also important to be precise about what we are discussing. You say that humans have intentionality and chatGPT doesn’t. Where does that intentionality for humans come from? Where in our neural networks does this capability for intentionality and emotion emerge? I point out these questions, because the way you describe how chatGPT works isn’t really fundamentally so different than how we work, and it learns in much the same way a human child would learn to speak a language."
">ChatGPT is impressive in its ability to mimic human writing. But that's all its doing -- mimicry. When a human uses language, there is an intentionality at play, an idea that is being communicated: some thought behind the words being chosen deployed and transmitted to the reader, who goes through their own interpretative process and places that information within the context of their own understanding of the world and the issue being discussed.

