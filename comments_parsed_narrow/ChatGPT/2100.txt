Fear and outrage are the strongest human emotions likely to keep you engaged.  Furthermore short 1-2 liners are likely to keep your attention.  So complex issues are reduced to headlines to keep you clicking and then your are pushed towards radical (conspiratorial theories) to trigger fear/outrage.    Business models with recommendation systems train themselves on what keeps a user engaged.   People don't tell it "make short headlines that trigger outrage to engage people".   They tell it "keep people engaged" and it learns that short headlines and outrage are the best way to do that.  So recommender systems end up radicalizing people and pumping out doom porn.   ChatGPT is seeing all this garbage and assuming this is "humans".   In a sense, it is, because we consume and regurgitate it, otherwise the recommendation systems wouldn't push it.   It's how the business models work, unfortunately.