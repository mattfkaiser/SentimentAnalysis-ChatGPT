A specific model could potentially do things like this. A LLM like ChatGPT could be included, for the purposes of interfacing with humans and human data, but there would have to be a specific model designed for science and technological advancement. Anytime there is a large dataset, a model can be trained. If there are datasets of the inputs and outputs of a system, a model can be trained to predict the outputs from the inputs with various degrees of accuracy, depending on variables like the size of the dataset, the computing resources devoted to training, the complexity of the system, etc. So if the inputs and outputs of science and engineering experiments are the dataset, the model could simulate experiments and predict the outcomes before the experiment is actually performed. It could design new experiments that maybe people wouldn't think of, to add to it's dataset in the most meaningful way. And a model could be specifically trained on logic and reason -- which current LLMs aren't very good at, because this is just an emergent property of current models -- so it could actually figure stuff out.