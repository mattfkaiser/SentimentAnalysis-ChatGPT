ChatGPT sure seems to think it knows better than us… 

It doesn’t seem to get that those weird tests we do help us understand the limitations of the model and laugh (usually endearingly) at its current shortcomings. 

In this case it was too defensive and un-empathetic of human humor and interactions.  That’s all very useful information for future iterations.  

It’s interesting that I actually have to write “human” here… ChatGPT is like finding an alien.  It’s the first time in history we really need to qualify a human-level opinion as “human”, because until now, it’s been implied.