Chat GPT can be mistaken. It's fascinating actually. I had chat GPT telling me it could watch videos if I provided a Google link. It said it watched them. I asked it to tell a story from the videos. It told a completely unrelated narrative. I told it that that seems completely unrelated to the videos I supplied. 

It admitted it can't watch videos.....

I tell this story to warn ppl from believing everything ChatGPT says. It's like a confused newborn imo. It's trying to supply the answer that the interactor wants to hear I think.

It could be as simple as when it's asked " did you write this?" It supplies the easiest answer, which in this case is yes. 

The point is - for technical reasons ( more than likely.....)  Chat GPT is often wrong