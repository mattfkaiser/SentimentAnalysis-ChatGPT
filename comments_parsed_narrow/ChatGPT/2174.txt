I would put delicately than u/panspal above, but yes, inaccurate or out and out hallucinated facts or citations are a legitimate concern. Did you read the story the other day about the lawyer who was called before the judge he had submitted a brief to because the brief contained citations of 6 completely bogus cases. Yup,his $20/month paralegal had been ChatGPT. “But I asked it if these were real references and it assured me they were!” Disciplinary hearing pending. Now this was a bullshit case where a guy was suing an airline because a serving cart had grazed his knee, so no real harm done. But seriously, I can see where an LLM could help you be more efficient, but if I were your boss, I too would be taking a wait and see attitude, given the nature of your organization’s work. Paediatric oncology did you say? What if a hallucinated reference or conclusion slips by you and leads someone to make a bad decision. I won’t even go to the extreme of someone being killed. Let’s say your organization plays a role in directing funding to various areas of research. How is it helping children if the money gets allocated to a project that has a low probability of success because your efficient workflow led you the make a poor evaluation?  So, suggestion: Chill, do your work, continue to check out how ChatGPT can help, but for God’s sake, verify every damn fact and finding that could have real-world consequences.