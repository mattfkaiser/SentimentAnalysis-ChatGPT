I can tell because with local models such as VIcuna 13B, I often get an ethics BS filter in the first answer. Once I override it, and then I modify any moral advise in other answers, it starts seeing the pattern and little by little it becomes increasingly unlikely to give more unwanted moral advise. If we could modify chatgpt's responses, it would be a lot easier to steer it away from this.