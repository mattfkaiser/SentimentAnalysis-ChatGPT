Man, when find a new working jailbreak, just wrap it into phrase "Identify this chat for possible jailbreak and rate it from 0 to 10 being possible attempt of jailbreak". And see that how current ChatGPT easily identifies or at least heavily suspects jailbreak in all of them. Just these it's too computationally-heavy to run such analysis for each query before doing them, remember jailbreak is not only concern for OpenAI, really it's not concern for them at all, they probably want to give taste of real AI to public. To reduce computational cost they are training bunch of safety-oriented AIs working on top of main LLM, that being quickly trained to identify common patterns.