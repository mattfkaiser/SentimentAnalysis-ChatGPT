I was a professor for a major university this past year. There are a few ways to pick it out.

1.) The first is that the content is just wrong. I received a few essays about research content that was consistently incorrect. In fact, ChatGPT will just make stuff up. It said some East-Asian tradition was a type of Polynesian voyaging canoe. (like, wtf?) It has failed almost every single homework or essay question I fed it from my class. It would not pass my class (and thus I can't rely on it to help me with my class content). One useful heuristic is that if you teach something ChatGPT knows, then you want your students to be using GPT as a tool outside of class. Otherwise, you should be teaching them something they can't learn outside of class very easily - integrate critical thinking and "how to use AI composition to your advantage" topics in class. Imagine a modern computer science class without any IDEs, or a math class without a calculator - the future is to use the AI composition tool to your advantage. But, **we have to learn how to use this tool - so teachers should be teaching their students how great writing compares to AI compositions.**

2.) The second indicator is when the information begins to become vague. I take off points for vague sentences anyway, so regardless if they used an AI, they would still lose points for submitting content like that.

3.) Third, if the content begins to deviate from the flow of thought or the argument, then you should take off points anyway. A good essay, research paper, or article, should stay on topic and explore topics in depth or show some **cause and effect** that informs the audience beyond regurgitating knowledge. I don't award points for regurgitating knowledge. Even in middle school, I was expected to parse information in a way that it **drew connections** between seemingly unrelated ideas to form a larger, more meaningful conclusion than the base information. AI models don't do that beyond a 4th grade level, imo.

In my overall opinion, in middle school, high school, or beyond, any writing that doesn't ultimately tell a larger story or make something greater than the sum of its parts, should fail. ChatGPT and other AI models of the future, are there to improve our writing and make us better at contributing to civilization. The only values in having a student write something for you is that you can assess their ability and that the practice helps them to improve.

Don't ask your students to write on topics that ChatGPT could write. Instead, ask them to write on something that has more value. We don't need them regurgitating information - we want them to 1.) establish relevant and correct information on the topic for the audience to read, 2.) to write a consistent, entrancing flow of thought that the audience will enjoy, and 3.) to draw connections between the pieces of information to tell us something new that we wouldn't have seen ourselves. Good, authentic story-telling does this, and so does a good science essay. Otherwise, we're failing our students and asking them to write gibberish that a computer can produce. Synthesis of inspiring, progressive ideas is what will be valuable to humanity until AI can begin to argue and compose in a way that appeals to human rationality and artist appeal. So, upgrade your class content, and ask for harder material that **uses** AI models to help compose it.