I've been surprised by chat GPT 4 hallucinating quite a bit today. I've previously only seen significant hallucinations from 3.5 and earlier. But I was asking it questions that are likely to be on the edges of its knowledge base related to some fairly obscure coding stuff that few people would ever come across or use. It was making up functions that don't exist. It never does that for more mainstream stuff though.