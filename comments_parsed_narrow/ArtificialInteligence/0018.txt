Well as you said, we don't have a proper definition for sentience. Without that, everyone is just going to be talking past eachother on the subject. I do think it's fair to say that LLMs are nothing more than language calculators though. We know exactly how they work, and they are deterministic, just like any calculator. Feed in the same input, and you will get the same output. That fact is largely obscured in services like chatGPT, whom inject random seeds to add variability to it's responses. However, if you run LLMs on any sort of local hardware, you can see for yourself, same seed, same prompt, same parameters, same output. They don't actively learn from experience or interaction. Tell it your name and it only remembers so long as that information lives in the context window which resets frequently. It doesn't ingrain that knowledge in its model. Their responses can be easily manipulated by simply suggesting the next sequence of characters. Prompt with "Two peas in a..." And it will likely answer "pod" depending on its training data. Suggest in a narrative voice that the AI is an abusive asshole, and it will suddenly start acting like one as if it always was. It can certainly speak about a lot of things, including sentience, but it certainly seems to lack a well defined sense of self. Many people have taken advantage of this to craft custom characters for their own personal AI assistants, all using the same base model. That implies to me that it's much closer to a calculator than a person. If you wanted to create something closer to human consciousness, I would imagine LLMs being a small part of that, but definitely not the whole thing.