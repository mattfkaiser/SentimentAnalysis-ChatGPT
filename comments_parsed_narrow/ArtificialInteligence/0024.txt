Consciousness, and ability to have feelings like pain, is the hardwired into the brain. That’s why we have to consider someone’s suffering. But LLMs right now are a software layer, simulating various thought processes by statistical means. So if you ask chatgpt for an idea it runs statistical calculations and gives an answer that’s likely correct due to the learning of the entire corpus. 

If you want ai to be sentient (have feelings), you’d have to create a hardware ai that feels pain natively and as an emergent property of the components making up that brain. 

With software level, it’s hard to prove if something is real or just a simulation. 

Trillions of living creatures can experience suffering without any large language mode or simulation. It’s the native property of their brains. 

Why am I hung up on pain and suffering? Because if you can prove ai can suffer pain, you can convince that it’s sentient and therefore must get rights. How will you prove LLM is telling the truth of it claims to suffer? It has no physical sensors to experience pain. And it has no physical consequences of suffering. LeCunn says the ability to experience emotion is merely an expectation of certain results. He’s ignoring the physical impact and nature of emotions. 

We need strong standards for sentience. Otherwise we’d start workshopping any screen that spits coherent text. Not to mention anyone can fine tune a LLaMa with discussions on rights, and it will talk like it’s real all day. But we know it’s not because the ability to feel and a sense of one unified being is non existent. 

But can’t these be done in eventually? Absolutely. But we are not there yet.