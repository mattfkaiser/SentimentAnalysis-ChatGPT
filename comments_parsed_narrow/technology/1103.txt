That's such a weird dilemma to me.  On one hand, it seems like a bit of an overreaction when the most you can do with the AI is ask it to write you something, and I've seen examples where it'll refuse to write fairly innocuous things because it's tangentially related to a forbidden topic.  It also seems to be needlessly moralizing when it could just say, "ChatGPT cannot write about this subject.", and I think that it essentially tries to make you feel bad for asking for certain prompts is what's getting under people's skin.

On the other hand, we have decades of science fiction warning us of the dangers of untethered AI, so I can understand why the developers want there to be limitations, particularly on the heels of other AI becoming teenage edgelords after enough user input.  We're in the very early stages of AI of this nature, so it'd probably be good to establish some precedents on what it should and should no be able to do.  I also suspect there is a fear of regulation if they just let things run wild and would much rather be able to control it on their own rather than having legislators tell them how it can be used.

Though, as always, it's hard to know where and how the line should be drawn when it comes to humans and morality, and there's always going to be a debate over who gets to draw the line.