Nah, the issue is deduplication. There is no compression algorithm capable of accurately representing all the training data within the weights of a model which is a miniscule fraction of its size. So, where sections of text are memorized, it is an indication that it is overtrained on that content, and it gets overtrained on that content because there are many copies of it in the training data. 

Who made all those copies? In most cases, not the copyright holder. You can find the vast majority of the Harry Potter series quoted on /r/harrypotter, some passages, many times. The corpus of reddit is part of ChatGPT's training data. Consequently, GPT-4 can give a pretty accurate retelling. 

So, should OpenAI be responsible for filtering out fair use quotations from copyrighted works? I don't think so. How would it do so without having access to those works in the first place? A better solution is just to filter out any duplicates of strings of a given threshold length. If that were done, it couldn't be argued that the AI isn't transformative, and isn't fair use. The hard part is, the smaller you make that threshold, the more computing power is required for deduplication, and the more likely that commonly used phrases are filtered out(which would negatively impact the accuracy of the model).