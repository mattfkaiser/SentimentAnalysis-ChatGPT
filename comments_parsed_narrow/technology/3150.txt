The problem with ChatGPT is that it's a really, really impressive language model.  It can do an incredible job... at generating really convincing text.  But that's all it's good at.  Notably, it can't reliably generate *true* text, because it can't differentiate fact from fiction.

It's like an actor that plays whatever role you ask it to.  It can write code by pretending it's a software developer by imitating the code examples it was trained on.  It can also write spells by pretending to be a wizard and imitating all the fantasy novels it was trained on.  As far as the model is concerned, both of those tasks are equivalent.  It has no way of understanding that with magic you can make up anything that sounds convincing because it's fictional, whereas software only actually works if you use real APIs that actually exist.

We can ask what the difference is between a software developer and an actor who perfectly imitates a software developer.  I don't know, but I *can* say that ChatGPT is *not* a perfect imitator.  It's pretty far from it -- it's good enough to write Hello World and to give you basic snippets.  If it's seen an algorithm in its training model, it can almost certainly regurgitate it faithfully (and possibly even in other programming languages, with varying degrees of correctness).  It can even combine these simple blocks into slightly more complex combinations.

But that's all.  That's the best it can do, because it only understands code as an odd language (in the linguistic sense) with a very rigid grammar and peculiar syntax, *not* as a sequence of instructions that when correctly arranged makes computers actually do things in the real world.

To make matters even thornier, ChatGPT is very good at *pretending* to know things.  It can give really thorough, comprehensive explanations about its supposed lines of reasoning.  If you ask it if it knows how to do all the things I just said it can't do, it'll probably give you a wonderful and convincing explanation of why it actually can.  But the fact that it rattles these very convincing arguments off to support obvious falsehoods as often as truths shows that it actually has no idea what it's saying.  It just knows how to sound like someone who does.