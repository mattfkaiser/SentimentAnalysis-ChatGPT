You don't need to mention neuron weights, just don't say it's storing answers (because it isn't generally). 

ChatGPT consumes a ton of data, examples of text. Each example it sees leaves a tiny impression on its "brain/memory," a shadow of the data. Across tens of millions of examples, many commonalities in the data are captured as the shapes formed by densely overlapping shadows. These commonalities are often recognizable *traits* of an example, like "tone" or "subject matter." Without storing any individual examples, the model learns a huge number of traits that an individual example might have, and can use this knowledge to describe a given example, and generate something similar to it.