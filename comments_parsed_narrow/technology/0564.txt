I actually don't think that's the way it will go. I think the future is going to be in better augmentation, and the "kernel" of a language model will be focused less on factual information, and more on systems of thinking. That "kernel" (which is a slot that ChatGPT can fit into, but very imperfectly) can be pretty generic, because it's not actually concerned with the information specific to any domain.

The reason I think this is that keeping models up to date with current information is both difficult and computationally expensive. As far as I know, there aren't any techniques for _removing_ information from a model that has already been trained. You can either delete the information and retrain from scratch (_incredibly_ expensive), or you can train on a bunch of new data that tries to nudge the model away from the inaccuracy (less expensive, but also probably less reliable).

On the other hand, if you have a very fast model which knows how to talk through and solve problems, retrieve information, and tie things together to reach a final answer, and you can build really effective augmentations that make it easy for the model to access that information, the problem is much, much easier. Now it's just a matter of curating the data the model has access to. You _never_ need to retrain, because the original training data is never out of date. You just have to make sure that your "kernel" has access to accurate information when it's working.