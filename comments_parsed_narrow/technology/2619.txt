It seems useful for some fairly mundane things. I was trying to remember a Czech soup I once had a recipe for. I knew it had the spice mace in it, and it seems weird but I couldn’t remember what sort of soup it was. I asked ChatGPT to find a famous Czech soup that had the spice in it. That didn’t work. Then I asked it for a list of famous Czech soups thinking that would jog my memory and it did. It was a cauliflower soup. So I asked it for a recipe and it gave me one. This is nice because most of the online ones are in Czech and it gave me English but the recipe didn’t have mace in it. So I asked it if it had a cauliflower soup with mace in it, and it just spit back the same recipe adding mace. I experimented with another recipe and saw the same thing. I have no idea if these recipes would work as yes, it seems to be bullshitting. I’ve seen it straight up get things wrong that have been web verifiable for over a decade. It said I was previously employed by Microsoft and while I worked with folks there, that’s not true and I’m not sure why it would think that. I know it will improve but what I see seems dangerous so far. It’s generating things that read fine and may be *almost* right.