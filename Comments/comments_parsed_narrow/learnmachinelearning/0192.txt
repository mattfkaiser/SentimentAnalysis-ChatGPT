As far as chatGPT goes, its answer is fine, I guess, perhaps a bit narrow. To simply say that positional encodings don't effect the semantics of word embeddings is fine to consider as True or False, but if you're going to say the semantics aren't effected you should probably focus on mentioning what does change, what improves, or else why even bother doing this? So its answer isn't foundationally wrong, just narrow. 



Point-wise addition is great in a lot of circumstances, that it deals with missing data is just a bonus. The best way to consider that specific benefit is considering how a "missing data" labels will have to work, compared to added embeddings. The critical thing is, even if you train a "missing label" embedding, it'll still be uniform across all unknown examples. With stacked embeddings, this won't necessarily be the case. However, point-wise addition of embeddings can be good even without any missing data. There's lots of perspectives that you can take on this, from considering how this effects the gradient, to how it deals with vector space mappings. Its difficult to pin down a single specific reason or condition where point-wise addition works.   


That said, I'm most often considering how the elements contribute to the semantics I think are reasonable, as well as the correlation between events- the qualities of the coincidence matrix. In the location sub-category type, you'd have a very very sparse co-incidence matrix between States and Cities, and I generally take this to imply that concatenating will not be viable. Adding is a good alternative here. Different embeddings, like Product Category and Location, seem more worthwhile to concatenate perhaps because their co-incidence matrix is relatively dense and higher population. I vaguely relate this to the number and diversity of samples each independent embedding vector is trained on. So you could train on D1xD2xD3xP1xP2, and have a very large low density space. Or, you could train on the space spanned by (D1+D2+D3)x(P1+P2), and have a smaller space with higher density, which probably helps generalization, i guess?