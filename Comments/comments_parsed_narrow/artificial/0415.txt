Strange opinion post.  
  
For all the people who thought chatGPT was already better than a human doctor, I guess this would be a rebuttal, but I don’t think I’ve ever heard anyone say that. It’s combatting an imaginary straw-man and winning an argument against no one.  
   
In terms of whether or not it’s dangerous to self-diagnose with chatGPT, that’s just, you know… stupid. Or incredibly naive.  
  
The glaring omission from this physician is that chatGPT is fast, free, and objectively safe. It takes minutes from comfort of your own home. You can not lose your house from chatGPT not being covered by your insurance. You can not get an infection from chatGPT or catch a pathogen in a chatGPT waiting room.  
  
You could, and he did, invent an imaginary danger of patients who may be overly confident in the diagnosis of chatGPT to the point where they choose to forego life-saving medical attention, but that would be like criticizing bandaids for the danger they cause by dissuading people from getting life-saving stitches when they have a severed artery.  
  
At the end of the day, people are in charge of their own medical decisions and hospitals, google, LLMs, and bandaids are all tools people employ to address their health concerns.  
  
If medical services were free, and doctors made house calls, you could conceivably make an argument that people shouldn’t consult chatGPT about medical questions.  
  
So long as professional medical services are inconvenient and expensive, people will exploit the easy free options first, and when people then choose not to seek medical attention, that says a lot more about the inaccessibility of healthcare than the overconfidence in chatGPT.  
  
Even at this early juncture of LLMs, he said that it made the correct diagnosis 50% of the time. So 50% of people would have walked away with the wrong diagnosis, but 50% would have saved $2,000.  
  
He also made his assessment based on 200-500 words. But, an actual patient, curious about their own condition, would presumably be much more elaborate than that.  
  
This edges into another unmentioned consideration that the hospitals spend very little time with each patient, and this is not apples to apples there either. This is using chatGPT in the context of the rapid-fire assembly line of a hospital, which people are already trying to avoid, and that’s part of it.  
  
So, generally, it’s a pretty biased and sensational article by a doctor who is clearly projecting insecurity in the light of this new technology.  
  
It’s like a metro system being criticized by a taxi company “yeah, the bus can get you where you want to go sometimes, but if such and such girl had used the bus, she’d have missed her flight!”  
  
Nobody is saying it’s better, but say it’s bad because it’s not as good without mentioning its other advantages is misleading.