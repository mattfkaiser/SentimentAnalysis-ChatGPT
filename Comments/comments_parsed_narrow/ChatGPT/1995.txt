Right. So being “an AI Model” or LLM (actually we interface with the model or models through ChatGPT, or the playground, or software we write to poll the API) …a model has inherent limitations and powers, same with the software. It’s not the magician, you’re the magician. 
There have been some enormously frustrating challenges in getting decent responses from day one. Some neat things we didn’t get from Google. And for those of us who work the model to get it to reveal its underpinnings, some very powerful connections or transformations. For a large number of “dumbers” I suspect it’s related to this, the tweaks made to discourage some unwanted transformations have impacted the quality of the responses to better fit to the quality of the prompt.