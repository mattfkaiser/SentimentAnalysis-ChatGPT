I also work in cancer data analysis. We use machine learning techniques for various tasks and discussed early on whether to integrate chatGPT into our workflows. We decided to wait, and not to do it yet. Here are the reasons:

1. ChatGPT does not guarantee your data is safe. And the last thing we want to do is leak out sensitive patient data in any shape or form, therefore any sensitive information is airgapped and can only be accessed by specific software where we either control the data through the whole process, or the company guarantees it in writing. ChatGPT is neither, so its an instant no go for any medical data analysis.
2. ChatGPT is trained on a large body of generic text. Reddit posts are actually a large chunk of its learning material, along with a huge body of unpublished books, and other similar heaps of text. Therefore it is good at forming coherent sentences and answering generic questions, but fails at anything detailed. For example, you can ask it to describe what colon cancer is and it will give a decent response. If you however ask it to describe the importance of macrophages secreting chemokines to remodel the extracellular matrix during colon cancer, then chatGPT will just give a vague, generic answer. It has not been fine tuned for medial data, therefore it is not useful for most domain specific tasks.
3. It is prone to hallucinate responses, which means every fact it produces needs fact-checking. This takes so much effort that it is often faster to use other methods of collecting facts, like reading a review paper.

Most of the above issues could be solved by having an in-house AI that runs on your own servers. This would be okay to use from a security perspective as patient data never leaves your servers. Secondly, it can be fined tuned by specialist data like in house models, knowledge bases or similar so it can give detailed responses in cancer (or any other field of interest). Third, configuring the model appropriately can make it focus on producing text with additional safeties enabled to make sure hallucination is reduced. This is usually at the expense of producing nice, flowing text but that is acceptable from a research standpoint.

Implementing an AI like the above is doable now, but at the current pace of development it gets outdated by next month. Therefore waiting an extra 6 months will greatly improve the quality of these frameworks and simplify the setup process, which is a better use of resources from a company standpoint. 

In conclusion, currently our standpoint is to use AI where it is already integrated into workflows to help with well-defined tasks like MS office or Github, and keep building internal test models to keep up with developments while we see rapid improvements. Once we get to the stage where we can reliably build specialised in-house generative AIs that perform well on company-specific tasks, then we will use it, but in our specific case we are not there yet. Therefore our advice is the same as your manager's: its nifty, but lets wait a bit before using it.