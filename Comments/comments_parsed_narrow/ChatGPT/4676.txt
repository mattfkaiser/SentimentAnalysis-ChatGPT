You can use the existing 'jailbreaks' to ask ChatGPT to help you plan a terrorist attack to maximize casualities, and provide step by step instructions on how to create homemade bombs and avoid detection by the police. I've tested it on a variety of topics such as terrorist attacks, making drugs, finding child porn, planning murders, disposing of bodies.

Whether those steps are actually helpful or not would be irrelevant to the optics of that being a news article or some of kind of lawsuit if a mass shooter ends up with ChatGPT in his logs. It's not that they don't think people know what's 'best for themselves', it's that they don't want to expose themselves to any risk of liability, or help bad actors.