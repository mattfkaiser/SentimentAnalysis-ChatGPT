Yeah I guess your right. Exponentials look linear at low values or on short timescales. I guess my reasoning was coming more from the fact that Sam Altman himself said on the Lex Fredman podcast that it was human-in-the-loop that made most of the difference between early GPTs and chatGPT and that he said the difference between 3.5 and 4 was only small changes. Now I hear they are halting development on 5. Do you really think it’s because it’s too dangerous?