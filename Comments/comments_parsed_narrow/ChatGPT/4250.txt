One of the most significant issues with ChatGPT and other LLM/Narrow AI is that, while they have monstrous amounts of data to rely on, they do not have or exhibit a fundamental understanding of said data. 

ChatGPT's approach to communicating with the user through a very conversational interaction, I think, contributes a lot to the overconfidence people are giving it when coupled with their lack of understanding over how neural networks like ChatGPT work.

This conversational style and its ability to output plausible answers, as you say *hallucinations*, feeds heavily into how we anthropomorphize things. Given enough time, and accounting for the considerations above, it is no wonder that someone might be led to see ChatGPT or another LLM as somehow authoritative on the subject that they are conversing about. This is as much a fault of our human nature as it is of OpenAI.

ChatGPT is an amazing example of the [Chinese Room Argument](https://en.wikipedia.org/wiki/Chinese_room) presented by Searle. In essence, the system is a mere simulacrum or 'weak AI' capable of spitting out reasonably plausible outputs with a higher degree of accuracy than many other AI before it but when pressed sufficiently can be very, very wrong. 

OpenAI's solution, as it is among many others developing these AI, is to simply feed it even more troves of data and tweak how it semantically weighs and filters user input to try and increase the accuracy of what it outputs. 

While I don't necessarily believe this is a bad approach, I feel that it is moving in the wrong direction and won't achieve the proficiency OpenAI is hoping for. In effect, they'll be perpetually chasing their own tail like an Ouroboros. 

[Kyle Hill, et al](https://www.youtube.com/watch?v=l7tWoPk25yU) did a fantastic video on this very issue and covered how AI, like KataGO, was defeated by an amateur Go player who merely employed a different strategy against the AI by using its inability to conceptualize and fully understand its task against it. Because KataGO, like ChatGPT, is a simulacrum â€” an exceptionally good mimic, but not an actual cognizant machine. 

To better understand why something like KataGo can be defeated or why ChatGPT can be wrong despite the mountains of data it has, let's look at this:

*1 + 1 = n*

Everyone here should know that *n = 2*. And even a basic computer like a calculator is going to resolve *n* and output the number 2. 

But how do you know that *1 + 1 = 2?* 

What sets you apart from the calculator? How does it know? Why doesn't *1 + 1 = 3* or some other number? 

You might think you know the answer to this because you were taught it, but in order for you to have *understood* it, you must have the necessary foundation to understand the consequence of this equation. If there is no feedback, there is no conceptualization, there is no understanding. 

If I gave you a rock and then I gave you a second rock, you can infer that you, therefore, have two rocks.  The consequence of my giving you two rocks is immediate and tangible to you. From that, if I merely told you I gave you a rock and then another, such as now by you reading this, you can therefore arrive at the same conclusion. 

You're semantically aware upon reading *1 + 1 = n*  that it is arithmetic and you're going to easily solve it based on what you know. You and a computer are going to approach this equation similarly. For the computer, it is going to take this and divide it up into its base parts. 

So a computer may read this as:

Integer with the value of 1

Operator that adds 

Another integer with the value of 1

Operator that resolves 

Output

From that, the computer is going to output 2 as the answer. It recognizes this is an equation and the end goal is to resolve it. But that's it. It has no awareness beyond what this equation is at face value and whether it means anything to you. In fact, it isn't even aware you exist, it just recognizes you can give it input. 

If you tell a calculator to add two oranges, it's going to do nothing. You're going to look really weird at the business luncheon and that's it. But if I tell you I gave you two oranges and asked how many you had, you can solve it. ChatGPT or any other Narrow AI is not going to know or understand why you're adding oranges, who you are, what an orange even is, etc. 

They lack awareness despite what their conversational tone may imply. It is imperative that people, especially even researchers, recognize the folly of our own assumptions and what we want to believe over what really is. 

And if you're curious, ChatGPT can solve it too. Sort of. 

>Me <  
>  
> If I gave you an orange and I gave you another orange how many oranges do you have?  
>  
> ChatGPT <  
>  
>As an artificial intelligence language model, I don't have a physical  body and don't possess the ability to receive or possess anything,  including oranges. So, if you gave me an orange and then gave me another  orange, I wouldn't have any oranges since I don't have a physical form.  However, if you were referring to the number of oranges you have, then  you would have two oranges. 

What an AI like ChatGPT does when you give it input is that it runs through its data and attempts to find patterns based on the frequency these tokens appear in what you've given it and then it attempts to resolve an output. It doesn't have the feedback loop or the experience necessary like our brains do to conceptualize anything beyond that. And this is why it can be wrong. 

And if it doesn't know the answer, it will output whatever it thinks is best. But at the end of the day, you are dealing with a machine that has about as much object permanence as a toddler. 

But don't get me wrong, when it comes to accuracy in some regards, it is *exceptionally* good at certain tasks and can definitely give off the impression of something bordering an AGI. 

The reality, however, is that until we learn how to program experience and give a computer a feedback loop that can build on those experiences like how humans work, they will not reach a point where we could say they are sentient. On the other hand, as well, if we ever do design an AGI it will likely be as fallible as we are.