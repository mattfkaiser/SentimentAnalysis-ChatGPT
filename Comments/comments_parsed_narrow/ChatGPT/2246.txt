I've done extensive testing and used GPT to make about 300 pages of curriculum and supporting docs. In that long process I tested what you are saying by establishing a baseline untouched chat as Control and tested all kinds of variations. Simply put, ask simple questions and get simple answers. You can ask it to elaborate on those simple answers in follow up questions and extract more detailed info but it doesn't volunteer it on the first pass. If you ask ChatGPT what makes a good prompt it will tell you. When you add more inputs like context, purpose and goal, restrictions and parameters on its methods, give examples, etc. Your results will be more accurate.