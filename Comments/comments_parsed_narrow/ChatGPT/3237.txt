Agreed! Next challange (there are many..) is the quality of training data. LLMs are perfectly capable of large-scale astroturfing, so there's a chance that 2022 was the last year that the internet could be considered a "clean" training set of largely human generated information. So now we might want to think about preserving the model weights from ChatGPT 4 as some kind of backup of how thing looked in 2022, before The Great Contamination. :)