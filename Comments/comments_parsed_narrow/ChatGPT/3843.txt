LLMs are trained on an enormous amounts of data. gpt3 was trained on millions of web pages. We don't know anything about 3.5 or 4 (both are used for chatgpt), but it's safe to assume that there are terabytes of training data. That's just text. That's A LOT of text. I doubt that there's enough books before 1000AD to make something that would be able to even keep a simple conversation.