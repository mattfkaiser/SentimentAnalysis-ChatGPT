IMO, we don't really know what would make ASI, ASI. Proven by the simple question, what is sentience? So we sometimes dismiss computing the next most likely response as being too simple to be related to the sentience needed to supposedly establish a software as an ASI. However, what if sentience is merely that? The ability to compute the next most reasonable response? What if you were to meet an AI like chatGPT in say, 10 years? That AI may have trained on near infinitely more data and perhaps developed into a software capable of producing the most reasonable response to any stimuli. It would be capable of acting exactly like a human minus the physical aspects. Would you really be able to thoroughly discount it as sentient? It would pass the Turing test. 

We are effectively training software to mimic humans. Trained to the limit, could it be possible that we create a perfect mimicry of a human personality? Is a perfect mimicry, a complete twin, an exact replica considered less than the original? How would we prove this either way?