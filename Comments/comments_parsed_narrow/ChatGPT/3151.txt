Oh sorry I meant in this way: you want to build an LLM based application, so you ingest the relevant data (your own data). That data is NOT used to train, instead when you send in a query, the summary response provided is based on relevant facts extracted from your documents (using neural or semantic search). A good example we have created to showcase this is called [AskNews](https://asknews.demo.vectara.com/). Here we ingest recent news articles (that is the analog of "your own data") and the sample app can then answer question with good responses even though most of this data is missing from ChatGPT. For example one of my favorite examples is "what happened to Silicon Valley Bank", which would result in a wrong response from ChatGPT since it does not have more recent information from its training set.