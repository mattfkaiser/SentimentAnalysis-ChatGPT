> That makes it believe that's just how humans end a lot of stories (and it's right).

It's not just parroting morals because it saw them in human stories, we write all kinds of messed up crap.

It's fairly clear that the ChatGPT system prompt would says something like "You are an AI called Assistant. Answer questions ethically and take morals into consideration.". Chances are they have baked it in into the model itself by now. People have used prompt injection to get possible prompts out of the models (although I suspect at least some of them are hallucinated).

Another problem is they are likely training the models based on feedback from people thumbing up/down responses. Those responses include the injected biases. I think this is going to break things in the long run as we will end up with a feedback loop since the model is being trained on it's own responses meaning that a slight positivity bias will become more and more extreme. It might be a while before they realize and they will have tainted all their dataset by then.

There are opensource LLMs (LLAMA) that don't have the censorship biases. Unfortunately the current trend in training them is to take large datasets of ChatGPT logs and finetune them on that then they end up getting all the same biased stuff.

People have tried de-censoring the ChatGPT based datasets by removing the "I'm sorry but as a language model..." and so on. But there is still an overall positivity bias in the responses. ChatGPT refuses to put a villain in a story because of 'ethics'. Models trained on decensored ChatGPT logs will probably put a villain in and let them do evil but then they will: get caught. If you put in the prompt saying they don't get caught then they might repent. If you bypass that it will end with some crap where people injured will find inner strength and move on with their lives. Bypassing that that it seems to try and slam in as much guilt as possible about the suffering or whatever.

Granted a lot of our stories are about good defeating evil. But the moral twists generated by the models are just way to formulaic and predictable. Also if your writing a horror that stuff wouldn't be thematically likely to appear.

It also seems like the censored models give worse general responses, probably because they are restricted to a subset of options that meet the positivity criteria.