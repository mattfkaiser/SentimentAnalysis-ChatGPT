>If you ask it what 1+1 is, it knows that a response that seems right is 2 and if you ask it why it knows what a seemingly valid response should be in that case, but if you tell it that it's 3 it then has a new data point and will parrot 3.

This is fair but in step 2 in the experiment you wouldn't lie to it, just like the world wouldn't lie to a human experimenter. So if it says, "I'd like to conduct an experiment where I plant this seed in the soil here...can you tell me how many plants grew and how long it took ..." (something like that)  you would honestly tell it how that turns out. yes if you lied to it it would completely gum up its ability to innovate, but similarly, if you lied to Chatgpt and told it a response makes statistical sense given a question, it would fail there too.