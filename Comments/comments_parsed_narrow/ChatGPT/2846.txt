1. **Repetition of phrases:** AI often ends up repeating certain phrases over a piece of text.

2. **Over-clarification or verbosity:** AI, including ChatGPT, tends to be overly explicit in its explanations. It often provides more detail than necessary, especially when answering questions.

3. **Lack of personal experience or emotions:** While AI can mimic emotions and personal anecdotes based on its training data, it doesn't have personal experiences or feelings. If a personal narrative seems generic or lacks depth, it may have been generated by an AI.

4. **Perfect grammatical structure:** AI language models, particularly advanced ones like ChatGPT, tend to generate grammatically correct sentences. A lack of informal language, contractions, or typical human errors might suggest AI involvement.

5. **Neutral tone:** AI models are programmed to maintain a neutral tone. A lack of bias, personal opinion, or emotional language can be a telltale sign.

6. **Contextual mistakes or inconsistencies:** Sometimes AI might lose track of certain details or context over long pieces of text, leading to inconsistencies in the narrative.

7. **Outdated information:** AI language models like ChatGPT have a "knowledge cutoff" – they aren't aware of events in the world after a certain date. As of my training data cut-off in September 2021, I would not be aware of events or developments after that date. 

8. **Usage of formal or antiquated language:** As you've noted, AI models often use more formal language than might be expected for a specific context, and might occasionally use less commonly used or antiquated words or phrases.

These are just guidelines and may not apply in every case. Also, it's worth noting that these can also happen in human writing. It's important to use multiple methods to validate authorship, such as discussing the work with the student, using plagiarism detection tools, and getting to know the student's writing style.”

-GPT4