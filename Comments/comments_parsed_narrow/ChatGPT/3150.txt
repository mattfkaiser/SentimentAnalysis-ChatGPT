I agree that ChatGPT as is does have this challenge of hallucinations (as they are called) - it can spit out a response that is incorrect yet seems so good. What we do at Vectara is called Grounded Generation which reduces these dramatically by allowing the response to be based on actual facts, and those citations (of where it took the information from) is provided to the user so that further fact checking can be done. I think that's where we need to head - something we can "trust but verify".