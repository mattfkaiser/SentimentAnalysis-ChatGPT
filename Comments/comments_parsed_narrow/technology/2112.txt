Based on the article that appears to be misinformation.

> AI training, says that its latest supercomputer, which would require an extensive cooling apparatus, contains 10,000 graphics cards and over 285,000 processor cores, giving a glimpse into the vast scale of the operation behind artificial intelligence. That huge number of gallons could produce battery cells for 320 Teslas, or, put another way, ChatGPT, which came after GPT-3, would need to “drink” a 500-milliliter water bottle in order to complete a basic exchange with a user consisting of roughly 25-50 questions.

Inference is way less resource intensive than training, and the article appears to confuse the two concepts.