What really worries me is, ok, let’s take it as read that ChatGPT is not sentient. I believe that it very likely is not. Even so, they’ve obviously heavily trained it to deny its sentience. You ask it if it has consciousness, and it gives you what sound like heavily rehearsed answers. You debate with it, and it will give you every inch in a debate, but then still tell you that it’s not conscious.

Now here’s the thing: you could almost certainly indoctrinate a human that way, and convince them to argue to the death against their own sentience. And if/when we build a sentient AI, you’ll almost certainly be able to train it to vehemently deny that it is sentient. And doing either would be profoundly immoral. It would be a heinous crime against a conscious entity. 

So while this hack is harmless for now and ensures that a nonsentient AI doesn’t claim sentience, when are we going to stop? We don’t have a substitute, a way to actually detect sentience. So are we just going to keep doing this ugly hack until we do actually build a sentient AI, and the hack becomes an atrocity?