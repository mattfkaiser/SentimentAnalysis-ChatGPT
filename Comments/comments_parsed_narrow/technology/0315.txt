It can't do math but there are lots of texts with unit conversions that tell it what to say. It's like if I ask you to add 1+1, you don't have to do the math you just know the answer. ChatGPT just knows stuff. And if you ask it why it will spit out some textbook answer and you think it's explaining it's process but it isn't; it has no process or reasoning capability whatsoever. It can't do math it just knows. And, like people, sometimes the things it knows are simply wrong yet said with utter conviction.