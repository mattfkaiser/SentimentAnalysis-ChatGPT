It's not optimism, it's extrapolation from a trend that has been consistent for 5 years. GPT-1 could barely form a coherent setence. GPT-2 could barely string a paragraph together. GPT-3 could write entire articles indistinguishable from a human, and GPT-4 is better than the majority of people at nearly any text base task, and significantly better than standard ChatGPT 3.5. 

These LLMS can take totally new input, correctly reason about it, and output something totally new that demonstrates understanding of the new prompt. These aren't just "stochastic parrots" that repeat what was in the training data, they can and do reason, although sometimes incorrectly. GPT-4 still struggles with advanced mathematics and programming, but given the rate of advancement and how many [emergent properties](https://arxiv.org/pdf/2206.07682.pdf) these models have demonstrated, it would be a mistake to assume they can't get any better.