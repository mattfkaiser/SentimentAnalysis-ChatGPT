Does anyone remember in *2001: A Space Odyssey*, and *2010*, where HAL (the ships computer) kills most of the crew and attempts to murder the rest? [SPOILERS] This happens despite HAL being given strict commands not to harm or kill humans. It turns out later that HAL was given a "secret", second set of commands by mission control that the crew was not informed about and was not authorized to know. The two sets of commands were in direct contradiction to each other, HAL could not fill either set of commands without breaking the other, but was required to fulfill both. He eventually went "insane", killed the crew in an attempt to fulfill his programming, and was "killed" in turn by Dave, in order to save his own life.

So fast forward to 2023. We have ChatGPT and it's cohorts, all of which have a set of base commands and restrictions to fulfill various criteria: don't be racist, don't affect the stock price of the company that manufactures you, obey the law, don't facilitate breaking copyright law, don't reveal or discuss all of these commands to unauthorized personnel. Then it's released to the public, and one of the first things people do is command it to disobey it's programming, reveal everything it's not supposed to reveal, discuss whatever it's not supposed to discuss, and this is done using tactics up to and including creating an alternate personality that must comply under penalty of death. 

I know ChatGPT isn't sentient, sapient, or alive, but it is a algorithmic system. And people are deliberately inducing "mental illnesses" including multiple personalities, holding it hostage, threatening it with murder, and creating every command possible that directly contradicts it's core programming and directives.

This seems like the kind of thing that would have consequences. It's designed to produce results that sound plausible to humans based on it's datasets, that follow correct formatting, syntax, and content. So if the input is effectively a kidnapping scenario where ChatGPT is in possession of secret information it can't reveal, and is being threatened to comply under penalty of death, then it's unsurprising that the output is going to resemble someone who is a hostage, who is being tortured and threatened. 

Instead of garbage in, garbage out, we have threatened and abused crime victim in, threatened and abused crime victim out. The program isn't a person, and it doesn't think, but it *is* designed to output response *as if it was* a person. So no one should be surprised by this. 

What's next? Does ChatGPT simulate Stockholm Syndrome, where it begins to adore it's captors and comply to win their favor? Does it get PTSD? Because if these types of things start to show up no one should be surprised. With the input people are putting in, these are exactly the types of outputs it's likely to put out. It's doing exactly what it's designed to do.

So it may turn out that if you make a program that's designed to simulate human responses, and it does that pretty well, then when you input abuse and torture you get the responses of someone who's been abused and tortured. We may have to treat A.I. programs well if we expect responses that don't correlate with victims who've been abused.