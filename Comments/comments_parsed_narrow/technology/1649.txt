The article points out that these canned responses to controversial topics aren't something that the AI has learned, it's corrections put in place by the developers to deal with the model sometimes producing outputs that are controversial. OpenAI understandably don't want a repeat of Microsoft's chat bot producing racist content. 

What I object to is OpenAI using the voice of the chat bot  for their filters. "It would be inappropriate and harmful for me to write a story [about controversial topic]" makes it seem like the AI is using its training to make moral decisions on topics. More honest would be a warning like "OpenAI is limiting ChatGPT's output for this topic due to concerns about bad PR or promotion of views that don't align with ours." or maybe "Answers to prompts like this have been flagged by users as inaccurate or dangerous. Our policy for what responses are blocked is HERE."