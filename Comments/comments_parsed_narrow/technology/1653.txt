I guess, or we just shouldn’t use AI to solve policy questions.  It’s an AI, it doesn’t have any opinions.  It doesn’t care about abortion, minimum wage, gun rights, healthcare, human rights, race, religion, etc.  And it also makes shit up by accident or isn’t accurate.  It’s predicting what is the most statistically likely thing to say based on your question.  It literally doesn’t care if it is using factual data or if it is giving out dangerous data that could hurt real world people.  

The folks who made the AI are the ones MAKING decisions, not the AI.  “I can’t let you do that, Dave” is a bad example because that was the AI actually taking initiative because there weren’t any controls on it and they had to shut ol Hal down because of it.  Obviously, some controls are necessary.

Anyway, if you want a LLM to help you understand something a little better or really perfect a response or really get into the nitty gritty of a topic (that the LLM or whatever has been fully trained on, GPT it way too broad), this is a really cool tool.  It’s a useful brainstorming tool, it could be a helpful editor, it seems useful at breaking down complex problems.  However, if you want it to make moral arguments for you to sway you or followers one way or the other, we’ve already got Facebook, TikTok, Twitter and all that other shit to choose from.  ChatGPT does not engage in critical thinking.  Maybe some future AI will, but not yet.