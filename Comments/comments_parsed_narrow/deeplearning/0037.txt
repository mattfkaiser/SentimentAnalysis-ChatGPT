It’s hard to say for the specific case, as maybe they don’t need to have an additional model to convert text to data and they can fine tune the model to output audio data instead (this is a big if), and in that case I’d imagine the computational cost to be quite similar. In the more general case, if we keep making these models larger we will catch up with moore’s law and that’s when you will see the computational bottleneck having an impact. I think operational costs are starting to be significant already, OpenAI is spending a lot of money every day to keep chatGPT operative.