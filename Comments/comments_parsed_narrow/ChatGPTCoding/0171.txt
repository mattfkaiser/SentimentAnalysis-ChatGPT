Well son, I understand well enough actually. There's a reason I use those weird things around the word "LIED" - because it's hard to do air quotes with your fingers in electronic communication.

Obviously ChatGPT didn't actively lie, or intentionally give me inaccurate information for any negative reason. This was simply the first time I've come across it being creative when the prompt was explicitly asking for factual data from a specific timeframe (a.k.a. "hallucinating).

I thought it was interesting, as well as the follow up conversation with it - so I shared.

 :)