He's not wrong. In certain hands, there's plenty potential for AI to spin out of control. It's not just people looking to cause trouble, it's also the curiosity that's been driving our lives for the last couple decades. We want to know how to make things better, faster, more efficient. So, ChatGPT can already write code, given decent prompts, right? I can't be the only one wondering what happens if it's given the ability to understand and update its own code. By dictionary definition, it'd be autonomous. We'd have one hell of an ethical debate on our hands, and that's best case scenario.