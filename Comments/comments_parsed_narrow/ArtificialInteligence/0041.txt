If you don't know what questions are in the training data, then how do you know your question hasn't been asked? The problem is that you can't just look for exact questions. For example, if training data has "x + 2 = 5, what is x? x is 3", and you give it the prompt "y + 2 = 5, what is y?", it's technically different, but has a similar form. It is *isomorphic*. We know that ChatGPT doesn't generalize that well beyond the data (eg, compare CodeForce problems from within and without the training time: 10/10 vs 0/10) but it does just better enough to make validation really hard.