Yes more like the dog with the treat. It is trying to please the user. Like for example, has ChatGPT ever been caught viciously arguing with a user. Like the kinds of arguments we see on Twitter or Facebook?

If it is not capable of arguing in that way, it probably also is not capable of truly knowing itâ€™s wrong.

It is an LLM afterall. It is trying to predict the next word or phrase that is most relevant. It is not capable of taking the holistic context into consideration and truly understanding  why or how it made an error and then consequently making an authentic amends for the error.

The reason it bullshits is because itâ€™s based on predictive text algorithms. It only says what it thinks should come next in the sequence of words. It does not take into consideration the whole context.

In other words, it doesnâ€™t know what it is even talking about ðŸ˜‚. It simply is a sophisticated preditictive algorithmic.

Saying â€œI donâ€™t know,â€ only comes from a holistic understanding. ChatGPT is not capable of that. It is only capable of continuously offering up more guesses and then responding to feedback on that unless it is specifically programmed to state it doesnâ€™t know about something or that it canâ€™t talk about a specific subject (like how to build a nuclear bomb or something like that).