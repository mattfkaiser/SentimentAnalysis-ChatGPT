For example, if an AI cannot allow a human to come to harm by inaction, then that means it must provide free healthcare for the human if they need it. Which is ""woke"" and ""socialism""."
"AI doesn’t need to gain conciseness (whatever that actually is) to destroy humanity, it just needs to reach certain level where it predicts everything we do (as it is currently doing incredibly well, ChatGPT is just predicting how a human will reply and does that pretty well, it doesn’t need to be correct because we ourselves are also often wrong, it just needs to sound believable), once there it only needs to reach a prediction of human destruction that it can enact, which is already in the human mind so reaching such conclusion is not that far fetched considering it just copy us, the very definition of self fulfilling prophecy, it does that because we told it to, not directly but with our general knowledge."
"ChatGPT, write me a dissertation on [entirely niche topic]."
ChatGPT: Here's a confidently incorrect dissertation on [entirely niche topic]. I made up 87% of it and will cite nothing.
"Chatgpt, make up some source documents for this project so I can upload them to a custom website."
Have chatGPT make the website for you while it’s at it.
"Chatgpt, what questions should I be asking you?"
My engineering professor in grad school said we could use chatGPT on our latest paper.
"ChatGPT doesn't easily generate anything NSFW, so all essays will be directly related to porn going forward."
"Using your knowledge of world history and ChatGPT-4, compare and contrast the Russian Revolution of 1917 and my (your one and only amazing teacher) asscheeks after eating too much Taco Bell™ late last night."
"Graduate student here, all of my exams in the past 2 quarters have been ""open ChatGPT/BARD"". It's just that, the material is so obscure and niche that the information you get is either straight up wrong, or not nuanced enough to count for a complete answer. It's painfully obvious when someone uses ChatGPT to solve a test, so everyone seems to be okay with it *for now*. In the future, when it continues to learn and become more nuanced? Who's to say.

