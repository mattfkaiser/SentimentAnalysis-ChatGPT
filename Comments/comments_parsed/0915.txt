Whatever concepts you asked it about, it does not understand them. The people whose wiring it is pulling from, do understand them, because they thought about it and then wrote the words. ChatGPT needs existing text to be able to generate new text. Which is why it is prone to mistakes. As an example, let's say it's trained on  data for a single a subject from 10 sources, and 1 of them is fabricated. It will retell that fabricated information just as it does the factual information, and even worse it will be interspersed. So it inadvertently ends up ""hiding"" false information with factual. And it has no idea, even if it's blatantly obviously to humans that it makes no sense. Why? Because that's what it does, it takes text and generates new text based on it. Does that seem like understanding?

