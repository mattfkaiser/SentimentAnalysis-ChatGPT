It's also often incorrect, and can give blatantly wrong information, or biased information (which there is plenty of). And it presents it very confidently, because it doesn't ""know"" it's incorrect. It only knows that ""this is probably the most correct way to string these words"", and if those words are false information, it does not know or care. Because, to put very on-the-nose, when you prompt ChatGPT, it is basically saying ""this is probably what you're looking for"". It has no understanding of concepts or hypotheticals. It cannot ""understand"" things. It just looks like it can. What it writes is nothing more computed language that's very realistic.

